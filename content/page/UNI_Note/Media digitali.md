+++
title = "Media Digitali  "
date = 2019-04-26T20:03:36+02:00
description = "RIASSUNTO MEDIA DIGITALI "
draft = false
toc = false
categories = ["IT"]
tags = ["after", "dark","media digitali"]
images = [
  "https://source.unsplash.com/collection/983219/1600x900"
] # overrides site-wide open graph image
+++




Esistono due tipi di comunicazione: una rivolta a molti (tipo Radio e TV) e una di tipo
privato detta P2P come il telefono o il servizio postale.
Il telegrafo fu inventato nel 1837 da Morse e Wheatstone, e il codice che si convertiva in
alfabeto fu inventato sempre da Morse. Esso è il primo esempio di comunicazione
simultanea.
E grazie a ciò si ha la possibilità di seguire a distanza un evento (politico, economico e
finanziario) proprio mentre si svolge. Le linee telegrafiche furono costruite o dallo stato o
da privati per trarne interessi.
A partire dalla metà dell’800 l’influenza del telegrafo sulla vita pubblica è stato enorme:
Si usava per dare ordini agli eserciti, notizie ai giornali, quotazioni di borsa ed ha avviato la
spinta verso la cultura della simultaneità. Tuttavia il suo utilizzo, è stato assai minore nella
sfera privata poiché non vi era intimità nella comunicazioni poiché gli impiegati telegrafici
leggevano tutti i testi e anche a causa del costo elevato del servizio.
Il telefono nasce nella seconda metà dell’800, ad opera di Antonio Meucci, Gray e Bell,
tuttavia fu quest’ultimo a depositarne il brevetto. Siccome in un primo momento il
funzionamento del telefono era incerto, i finanziatori di Bell offrirono il brevetto alla
Western Union (l’azienda che aveva il monopolio dei telegrafi ) la quale però rifiutò. L’anno
dopo però ci ripensò e si mise in concorrenza con la Bell che però la citò in giudizio per
violazione di brevetto vincendo la causa. La Bell nel 1885 creò la T&T ancora oggi una
delle più grandi imprese al mondo. Il telefono ebbe così larga diffusione tra l’utenza
privata risulto molto più pratico del telegrafo.
La commutazione automatica: In primo momento per chiamare una persona, ci si doveva
mettere in contatto con una centralinista, la quale passava poi la chiamata al destinatario.
La commutazione automatica fu inventata da Strowger, consisteva in un circuito
elettromeccanico che sostituiva l’intervento del centralino. In oltre era il primo sistema ad
integrare un conteggio automatico della tassa detta billing che l’utente doveva pagare in
base alla durata della telefonata. Sussisteva però il problema che con le lunghe distanze il
segnale telefonico si indeboliva creando disturbi alla chiamata. L’invenzione del triodo
sopperì a tale problema.
Il Teletext venne inventato da Baudot nel 1874 che superò il tasto unico del telegrafo con
un codice di sua invenzione fondato su 0 e 1, con 5 tasti riusciva a rappresentare tutto
l’alfabeto, dalla parte del ricevente una macchina decodificava il segnale stampandolo su
carta.
Telefoto e fax La telefoto viene inventata nel 1907 da Belin e viene utilizzata ampiamente
nell’industria editoriale, utilizzata la linea telefonica per l’invio di fotografie. Il successoredella telefoto sarà il fax che sarà diffusissimo a partire dal 1970 anche in ambito
domestico.
Il telefono come servizio universale : Vail, direttore della Bell Company, sfruttava parte
dei ricavi della società per investirli per migliorare le infrastrutture nei paesi più poveri e
quindi con meno possibilità di guadagno. Grazie ai suoi accordi con il governo americano
Vail si garantì il monopolio della telefonia americana. Nel 1925 con i proventi della sua
azienda Bell istituì un laboratorio di ricerca da cui uscirono invenzioni come il transistor e i
satelliti per le telecomunicazioni.
Il telefono in Europa e Italia: In italia lo sviluppo della rete telefonica fu lento e
travagliato, lo Stato teneva per sé i collegamenti interurbani, mentre quelli urbani erano
gestiti da 5 società diverse. Nel 1962 la SIP ingloba tutte le 5 società, e solamente dal 1970
è possibile la “teleselezione” cioè non si deve più passare tramite il centralino per
chiamare persone residenti in altre città. Questo sarà possibile tramite i prefissi che
diversificheranno il luogo di residenza. Nel 1992 la SIP diventa Telecom Italia.
Reti telefoniche e televisione: Negli anni ’50 le linee telefoniche sono servite a
trasportare il segnale radiofonico e televisivo, successivamente, durante gli anni ’60, si
aggiungeranno ad esse i satelliti artificiali.
La prima trasmissione televisiva in mondovisione è compiuta da Telstar, il primo satellite
per le telecomunicazioni. Questo però era operativo solo qualche ora al giorno, quando la
sua orbita lo poneva in una posizione utile da poter ripetere i segnali. Per ovviare a questo
problema, e potendo ciò utilizzare permanentemente i satelliti furono inventati i satelliti
geostazionari, posti a 35.786 km dalla Terra.
Essi, collocati a tale distanza girano intorno alla Terra nello stesso periodo in cui essa gira
intorno a se stessa, apparendo in tal modo immobili ad un osservatore terrestre. Dal
satellite invece, si vedrebbe sempre la stessa porzione di Terra. Un satellite geostazionario
copre circa 1/3 della superficie terrestre, dunque ne servirebbero 3 per coprire tutta la
Terra, salvo alcune zone a ridosso dei poli.
La telefonia cellulare: La telefonia cellulare nasce nei laboratori Bell nel 1946 e ha scarsa
diffusione per gli elevatissimi costi d’impianto e per la complessità del servizio. I telefoni
cellulari funzionano grazie allo stesso principio alla base delle trasmissioni radiofoniche e
televisive, ovvero utilizzando le onde radio. Una tipica rete cellulare è costituita da uno
schema a celle esagonali contigue contenenti ognuna antenne. A ogni antenna viene
assegnata una determinata frequenza e quindi ogni cella ha una frequenza differente
rispetto a quelle adiacenti, questo per evitare interferenze. La potenza utilizzata per i
segnali radio è debole, cosicché le frequenze usate nella cella ‘A’ possono essere
riutilizzate nella cella ‘X’ non limitrofa, senza reciproci disturbi tra le due celle.
Come regola generale ogni terminale mobile si connette con l'antenna da cui, in linea
d'aria, dista meno. Quando quindi ci muoviamo verso il bordo che separa due cellecontigue, quello che accade è che ci spostiamo verso una zona in cui almeno due diverse
antenne sono da noi equidistanti. Una volta valicato il bordo e passati nella cella adiacente
sarà la nuova antenna che presiede la nuova cella a connettersi al nostro terminale mobile,
in applicazione della regola generale espressa in precedenza. Ci sono però altri casi in cui
la regola della connessione all'antenna più prossima viene infranta e pertanto il nostro
terminale si ritroverà connesso con un'antenna diversa rispetto a quella più vicina. Alcuni
esempi possono essere il fatto che una certa antenna è congestionata a causa della
presenza di troppi terminali mobili all'interno della cella da essa sottesa (caso tipico:
grandi raduni di persone come concerti, manifestazioni, ecc.).
Diversamente da quanto accade per le trasmissioni radio-televisive, nelle comunicazioni
mobili le onde radio sono trasmesse in entrambe le direzioni, in modo da collegare il
telefono cellulare alle stazioni radio base vicine e viceversa.
Solo negli anni ’80 si creano le condizioni ideali per l’avvento del cellulare: accresciuta
domanda di reperibilità, maggiore competizione tra gli operatori di telecomunicazioni,
microprocessori a basso costo ecc.
Il GSM:
Il GSM è un sistema di telefonia mobile digitale lanciato in Europa nel 1991. Il telefono
cellulare GMS sostituisce progressivamente i vecchi sistemi di telefonia mobile analogica
come la rete TACS in Italia ( quest’ultima aveva molte limitazioni, quali il poter trasmettere
unicamente messaggi vocali, ridotta capacità di banda e poteva essere usata solo in Italia).
Il nuovo sistema GMS consente ai possessori dei cellulari di spostarsi in Europa anche al di
fuori dei confini nazionali senza dover cambiare telefonino. Lo standard GSM permette
l’invio di sms (short message service), brevi messaggi testuali con non più di 200 caratteri.
Pensati inizialmente come sistema di comunicazione di servizio per gli operatori della
telefonia mobile, diventano rapidamente un fenomeno di costume. Nokia, che ha assunto
la guida per la telefonia cellulare (mentre Motorola quella analogica), ha sviluppato anche
il sistema di scrittura intuitivo T9. L’impatto di questa tecnologia in Italia ha provocato la
fine del monopolio pubblico SIP/Telecom, possibile dunque la concorrenza, a garanzia di
un mercato più libero. Pochi anni dopo arriva il momento del GPRS . Con la terza
generazione o 3G arriva il momento dell’UMTS e della trasmissione dati ad alta velocità
anche per i dispositivi mobili.
Roaming : Il roaming indica la capacità di cellulari di connettersi a reti differenti da quella
del proprio operatore mobile. In Italia tra gli operatori telefonici “reali”, cioè quelli che
hanno una propria rete installata sul territorio italiano (Tre, TIM, Vodafone e Wind), solo Tre
utilizza il roaming dati nazionale in quanto fornisce solo una copertura del segnale UMTS
(3G), mentre affida la connessione GPRS (2.5G) ad altri operatori e questo naturalmente ha
dei costi che esulano dal proprio piano dati.UMTS:
Dopo l’avvento del GSM sono stati introdotti vari standard migliorati, in particolare per
assicurare la connessione ad Internet, scattare e inviare foto e video (purché brevi): WAP,
GPRS, EDGE.
Un’efficiente e rapida connessione ad Internet, e una gestione assai migliore dei video più
lunghi, sono state assicurate solo dalla terza generazione dei cellulari, l’UMTS, che ha
permesso anche altri servizi: i messaggi multimediali, contenenti foto, clip audio o video;
la registrazione e la visualizzazione di video; il download di file musicali; le videotelefonate.
In Italia nel luglio 2000 viene pubblicato il bando per attribuire cinque licenze nazionali
UMTS.
Vincono Tim, Vodafone, Wind, Andala e Ipse. Andala ha già ceduto tutto al gruppo
Hutchison Whampoa: il nome cambia subito dopo in H3G, poi semplicemente 3.
Ipse non parte mai e chiude nel 2001 sostenendo che, dopo l’11 settembre, non c’è più
mercato per un nuovo operatore.
La prima generazione dei telefonini UMTS delude gli operatori perché:
• Nessuno è rientrato dei soldi spesi per l’asta 2000-2001;
• Non c’è un modello di business;
• La visione della tv dal telefonino e la videochiamata appaiono promettenti, ma si
scontrano con forti limiti fisici e sociali;
• Il collegamento a Internet è ancora tecnologicamente precario e non c’è ancora il
Wi-Fi a cui connettersi (risparmiando sui costi);
• Si aspetta una successiva generazione con una multimedialità più affidabile e
piacevole.
Come abbiamo visto precedentemente, gli inizi dell’UMTS (2002) sono deludenti: la tv sul
“tvfonino” e la videochiamata sul “videofonino” non riscuotono il successo sperato. La
videochiamata sarà totalmente assorbita da Messenger e Skype. Nel 2006 compare il
primo “smartphone” UMTS, il BlackBerry, dedicato all’utenza affari e di fascia alta. Nel
2007 viene lanciato l’iPhone di Apple e presto lo smartphone assumerà un carattere
spiccatamente multimediale, creativo, partecipativo e ludico. Gli anni successivi vedono la
competizione tra Apple e Samsung (sui dispositivi) e quella tra Apple, Android-Google e
Windows Phone per quello che riguarda i sistemi operativi, mentre si diffondono i tablet.
Wireless, seamless : L’integrazione fisso-mobile è destinata a crescere: il concetto di cui
ora si parla è “seamless”, senza cucitura. La prima tecnologia senza fili dopo il cellulare è
stato il Bluetooth, lanciato dalla Ericsson nel 99 per connettere tra loro senza fili apparati
elettronici (computer, microfoni, cellulari e periferiche) e scambiarsi informazioni. Unostandard dai costi molto ridotti che usa le onde radio, il cui segnale copre però solo un
raggio di pochi metri in ambienti chiusi.
Nel 2002 è invece nato il Wi-Fi, una tecnologia di rete senza fili che arriva a qualche decina
di metri e utilizza frequenze libere e gratuitamente disponibili. Serve per connettere senza
fili apparati elettronici nelle diverse stanze di una casa o ufficio a costi molto ridotti, ma è
stato convertito nella fornitura di accessi a Internet per computer, cellulari o palmari che si
trovino in prossimità di un access point, che può essere anche pubblico (hotspot).
Quest’ultimi possono essere gratuiti o a pagamento, e sono diffusi in aeroporti, alberghi,
biblioteche, parchi pubblici, e anche esercizi commerciali.
Cinema e televisione
Nei primi cinquant’anni del Novecento, il cinema ha costituito l’unico dispositivo per la
visione delle immagini artificiali in movimento.
Nella seconda metà del secolo il cinema ha convissuto con la televisione.
La televisione è figlia della radio. Proprio con la radio il cinema realizza un patto non
scritto di spartizione : alla radio toccava lo spazio privato, al cinema quello pubblico. Nel
1928 il cinema diventa anche sonoro e minaccia di invadere i territori della radio, fra cui
quello più prezioso ovvero la musica. Per correre ai ripari si inventò la televisione, la quale
comparsa tuttavia fu definitiva solo nel dopoguerra.
La televisione dimostra di avere un grande richiamo popolare, al punto tale che svuota
ristoranti e cinema e ogni luogo di ritrovo è costretta a installarne una. Anche la
televisione è un esempio di simultaneità e ubiquità ( porta gli eventi ovunque).
Il cinema ha un carattere prevalentemente narrativo, la televisione invece oltre che essere
narrativa è fonte di informazioni ( testimoniale ) e conversazionale.
Come la radio, per la televisione esiste un modello americano e un modello europeo:
• Modello americano: canali gratuiti di imprese commerciali che si ripagano con la
pubblicità. Curvato sull’intrattenimento per attirare più spettatori possibile e quindi
tenere alte le tariffe pubblicitarie.
• Modello europeo: canali pubblici, in regime di monopolio, in cui la pubblicità non
c’è o ha valore accessorio, con una missione “pedagogica”: “Educare, informare,
intrattenere” secondo le parole di J. Reith primo direttore della BBC.
La scelta europea non era solo culturale ma di necessità: in nessun paese europeo
esistevano le condizioni economiche per il modello americano; Appena ci sono state, il
modello europeo si è dissolto.
InternetLe origini di Internet risalgono agli anni '60, in piena Guerra Fredda, quando il mondo è
diviso in due grandi sfere d'influenza (USA-URSS) ed incombe il terrore di una guerra
nucleare. Il Ministero della Difesa americano, in continuo allarme per la minaccia
sovietica, incarica l'ARPA (Advanced Research Projects Agency) di studiare un sistema di
rete, in grado di preservare il collegamento via computer tra le varie basi militari in caso di
guerra nucleare.
Gli studiosi partono dalla convinzione che l'unico modo per assicurare la continuità nella
comunicazioni sia quello di prescindere da un nodo centrale la cui distruzione avrebbe
compromesso il funzionamento dell'intera rete.
Nasce, così, una rete decentralizzata, denominata Arpanet, studiata in modo che ogni
nodo potesse continuare ad elaborare e trasmettere dati qualora i nodi vicini fossero stati
danneggiati.
La rete Arpanet cresce a vista d'occhio basandosi su un sistema di protocolli, TCP/IP
(Transmission Control Protocol/Internet Protocol). Il cresente utilizzo porta, nel 1983, alla
creazione di due reti, la prima, prettamente militare, prende il nome di Milnet. La seconda
denominata Internet, dal nome del protocollo principale, viene regalata dall'ARPA alle
Università e inizia a diffondersi nelle altre sedi americane ed europee oltre che nei più vari
Centri di Ricerca, che ne fanno proficuo uso.
Le reti cominciarono a connettersi fra loro adottando protocolli comuni e utilizzando le
comuni linee telefoniche grazie ai modem (MODulator-DEModulator).
Una rete segreta militare si trasforma dunque in una rete mondiale, pubblica, gratuita.
Non ci fu bisogno di creare reti di telecomunicazione: la fortuna di Internet fu di
appoggiarsi alle già esistenti reti telefoniche.
Nel 1990 al CERN ( Centro Europeo per la Ricerca Nucleare di Ginevra), alcuni ricercatori
informatici, Tim Bernes- Lee in primis, incaricati di realizzare un sistema per la condivisione
tra utenti diversi di dati sia testuali che non testuali ( immagini, suoni. Filmati), basandosi
sul concetto di ipertesto hanno dato origine al linguaggio HTML che consente, oltre a
gestire informazioni di diversa natura ( testuali e multimediali ), anche di collegare diversi
documenti tra loro mediante opportuni link. Questo linguaggio è divenuto lo strumento
più potente per distribuire informazioni in internet ed ha introdotto quella architettura
denominata WWW ( World Wide Web), la ragnatela di dimensioni mondiali che consente
la navigazione, cioè la consultazione semplice e veloce degli archivi e dei documenti
presenti nei computer della rete.
Un altro protocollo per il trasferimento delle rappresentazioni così formattate è l’ HTTP .
NB: Un protocollo è un insieme di regole e procedure da rispettare per emettere ricevere
dei dati su una rete. Ne esistono differenti secondo quello che ci si aspetta dalla
comunicazione. Per internet si usano i TCP/IP e si basa sulla nozione d'indirizzamento IP,
cioè il fatto di fornire un indirizzo IP ad ogni terminale di rete per poter inviare deipacchetti di dati.
Su internet, i computer comunicano fra loro grazie al protocollo IP (Internet Protocol), che
usa degli indirizzi numerici, detti Indirizzi IP, composti da 4 numeri interi (4 byte) compresi
tra 0 e 255 e siglati sotto la forma di xxx.xxx.xxx.xxx. Ad esempio 194.153.205.26 è un
indirizzo IP dato in forma tecnica.
Questi indirizzi servono ai computer di rete per comunicare fra loro, quindi ogni computer
di rete ha un indirizzo IP unico sulla rete stessa.
Nel 1993 due studenti dell’Università dell’Illinoise realizzano il software Mosaic, il primo
programma di navigazione per non esperti, diffuso gratuitamente, ma comunque
all’interno di una cerchia ristretta di accademici e ricercatori. L’anno successivo Marc
Andressen lancia Netscape, il primo browser commerciale, che sarà poi sostituito
dall’Explorer di Windows95. Internet finalmente esce così dalla ristretta cerchia degli
addetti ai lavori.
Morfologia di internet: La realizzazione del WWW fa nascere un oggetto mediale del tutto
nuovo. La pubblicazione di un sito è un atto pubblico, politico, esposto alle reazioni degli
altri. La grande forza di Internet è unire insieme, in modo accessibile, una forma di
comunicazione pubblica (i siti web) e una forma di comunicazione privata, che è
rappresentata dalla posta elettronica. Queste due dimensioni prima erano incarnate in
media differenti (es spot in tv per comunicazioni pubbliche e telefono per comunicazioni
private), ora non più. Con Internet la soglia di accesso alla comunicazione in pubblico si
abbassa a favore anche della gente comune.
La gratuità e il carattere paritario sono dunque elementi fondanti di Internet.
I siti web: Il World Wide Web, comunemente chiamato “il Web”, è un immenso insieme
di documenti, testuali e non, interconnessi tramite hyperlink, o semplicemente link. Da
non confondere con Internet, che è invece una rete di computer. un sito Web è una
collezione di documenti e pagine web correlati e raggruppati sotto un unico nome.
Chiunque voglia creare un sito web (definito tecnicamente anche “dominio”) deve
innanzitutto trovare un nome attraente e semplice da ricordare, descrittivo del contenuto,
che sarà collegato all’indirizzo IP assegnato al sito. L’uso del nome (DNS) ha semplificato
molto la navigazione. DNS è l’acronimo inglese di Domain Name System (sistema dei
nomi a dominio).
Prima del 23 giugno 1983, ovvero quando fu ideato il DNS, ogni computer o server
connesso a internet, era raggiungibile solamente digitando il suo indirizzo IP.1 Quindi per
collegarsi a un sito internet prima era necessario conoscere l’esatto indirizzo IP.
Per farvi un esempio, per collegarsi a Giardiniblog, invece di scrivere nel vostro
browser www.giardiniblog.com, una volta avreste dovuto scrivere 188.121.50.96 che non è
altro che il rispettivo indirizzo IP del nostro sito.
Bisogna poi trovare un “web host”, cioè una società commerciale che a pagamento ospiti
il contenuto del nostro sito sui suoi server, e poi procedere alla registrazione.
Riassumendo ci sono tre passaggi fondamentali : creazione del sito, caricamento del
sito sul server (FTP), hosting del sito da parte della società di server.La posta elettronica: Inviando un messaggio di posta elettronica a un interlocutore,
tramite un programma di posta elettronica (Outlook Express, Eudora, Thunderbird) esso
viene recapitato al server del destinatario che lo conserva finchè non lo scarica sul proprio
computer. E’ quindi un tipo di comunicazione asincrona, che permette una risposta in
tempo differito. Il fatto di avere più indirizzi ( uno lavorativo, uno personale, uno per casa
ecc) e di assegnare ad ognuno una parte limitata della nostra personalità, rappresenta una
moltiplicazione di se stessi. Ciò ribadisce la collocazione della posta elettronica nella sfera
privata.
I media sonori digitali
Per “suono digitale” si intende qualsiasi brano musicale, o sequenza di suoni, rumori,
voci, che sia convertito in forma numerica. La conversione del suono in digitale si fonda su
un processo di campionamento dell’onda sonora attraverso una funzione matematica. La
funzione può essere segmentata in frammenti abbastanza piccoli da poter esser
considerati singoli punti, ciascuno dei quali identificato dai suoi valori su un asse
cartesiano ed è rappresentabile attraverso i numeri.
La successione di questi valori (la frequenza di campionamento) misura l’efficienza della
nostra trascrizione: più è breve l’intervallo tra una campionatura e l’altra, più la
digitalizzazione è accurata.
Formato mp3: Il fenomeno del MP3 ha rivoluzionato il mondo di Internet che non è più
stato lo stesso. È un file di tipo lossy cioè un file compresso con una perdita di qualità.
Il formato MP3 elimina ciò che l'orecchio umano non riesce a sentire.
Questi suoni vengono eliminati ma ne viene lasciata una piccola parte in modo da non
rendere
"drastico" il taglio.
l modo nel quale le porzioni sonore sono eliminate dipende anche dal bitrate, stabilito
dall' utente al
momento della compressione. Il bitrate corrisponde al numero di bits per secondo usati
per la
memorizzazione del file. Più alto sarà il bitrate e più alta la risoluzione sonora.
Immaginate un filmato: con più fotogrammi l'immagine sarà fluida, allo stesso modo ad
un bitrate
maggiore corrisponderà un suono più completo, fedele all' originale.
Musica gratis: Napster, il primo sito per lo scambio di musica in filesharing, appare
nell’autunno 1999 creato da Shaw Fanning.
E’ una transazione p2p tra privati, senza l’intervento di un server. La Riaa nel 99 gli fa causa
per infrazione dei diritti del copyright. Le pratiche di filesharing gratuito diminuirono,
anche per l’arrivo di nuovi attori come iPod e iTunes di AppleCon la diffusione dello straming e del filesharing la musica perde il conatto con un
supporto materiale, che l’aveva accompagnata nel 900. La musica diventa sempre più
un’arte riprodotta, registrata, accessibile a tutti, e il suo consumo si sposta sempre più dai
luoghi pubblici a quelli domestici e infine negli spazi aperti grazie a radio e walkman.
Con lo streaming e il filesharing la differenza tra musica istantanea ( quella in diretta,
trasmessa dalle radio,quindi effimera, non registrabile, caratteristiche della musica dal
vivo) e permanente (quella delle collezioni di dischi, la musica dentro casa, istantanea)
viene ridefinita.
L’iPod e il “Digital Rights Management” Mentre Napster chiude, nell’ottobre 2001 Apple
Computer lancia l’iPod, un sofisticato lettore audio e video portatile che porta con se una
nuova idea dei rapporti tra i detentori dei diritti sulla musica e e i diritto dei consumatori.
Apple afferma di perseguire una terza via tra le crociate dei discografici contro la pirateria
e la musica gratuita di Napster, quella del prezzo equo:realizza accordi con le cae
discografiche spuntando condizioni ,migliori (99 cent per una canzone) e inaugura il lancio
di iTunes Music Stores
Il podcasting : Che cos'è il podcasting?
Il termine podcasting (Personal option digital casting) indica un sistema innovativo di fruire
i brani audio e video pubblicati su internet. Non c'è più bisogno di collegarsi ad un sito ad
un orario prestabilito, né di cercare e scaricare i file uno a uno.
Usare il podcasting è un po' come essere abbonati a una rivista: i contenuti arrivano
direttamente nel computer e lì restano a disposizione, per essere ascoltati, distribuiti o
copiati in un lettore portatile.
Come funziona il podcasting?
Il podcasting funziona su periferiche portatili come la Sony PSP, l’Apple iPod o simili
tramite il sistema integrato di gestione dei FEED, o su tutti I normalissimi PC grazie a un
software gratuito che, a intervalli regolari, si collega a internet e legge quali audio sono
stati pubblicati dai siti ai quali si è abbonati. Se ne trova di nuovi, li scarica sul computer.
La crisi dell’11 settembre : Ha messo in crisi e quasi bloccato lo sviluppo di internet,
poiché era stato sostenuto che il web avesse aiutato i terroristi a reperire informazioni e a
tenersi in contatto.
Le nuove esigenze di controllo privilegiano la sicurezza, non più la velocità. E’ cambiato il
clima: è caduto del tutto l’ottimismo tecnologico. Finisce l’idea che, dopo il crollo del
muro di Berlino, il mondo fosse entrato in una fase di pace e di cooperazione tecnologica.
Economia del dono : Oggi usiamo Ebay e Amazon che non vendono più solamente
oggetti ma anche una “reputazione”. Scrivendo una recensione su Amazon non guadagno
niente ma ho la sensazione di fare qualcosa di utile.
Recensire on line un acquisto, scrivere una voce di enciclopedia, creare software libero,
sono tutte forme di economia del donoWeb 2.0: Nel web 2.0 c’è una partecipazione etica e disinteressata, ma anche un
sostanzioso aumento del commercio elettronico. Il punto più delicato del commercio
elettronico è la transazione in denaro. Essa generalmente avviene tramite una carta di
credito, protette tramite l’impiego di certificati digitali SSL (Secure Socket Layer). Tali
sistemi sono generalmente accessibili attraverso password o PIN.
Dalla metà dello scorso decennio l’affidabilità dei sistemi di sicurezza ha raggiunto un
livello maturo.
Coda Lunga : per un bene digitale immagazzinato online non ci sono costi di
mantenimento né di eventuali possibilità di usura.
GPS:
Attualmente, l’unico sistema di navigazione satellitare e posizionamento globale
effettivamente in funzione è il GPS (Global positioning system) concepito negli Stati Uniti
a scopi militari negli anni Settanta. Esiste anche un analogo ex sovietico, il GLONASS, il
cui sviluppo è stato a lungo interrotto: ma l'Agenzia Spaziale Russa lo ha recentemente
ripreso e al momento è in fase di completamento. Ne esiste anche uno europeo di
lentissimo avvio, il Galileo.
Il sistema GPS è costituito da una rete di satelliti che orbitano nello spazio a una distanza
ben precisa dalla terra e inviano segnali ai ricevitori GPS al suolo. A questi segnali sono
associati un codice temporale e dati geografici che consentono agli utenti di tutto il
pianeta di stabilire con esattezza la propria posizione, l'ora e la velocità di marcia.
Il Global Positioning System (GPS) è gestito dal Dipartimento della Difesa statunitense e si
articola in una costellazione di 24 (massimo 32) satelliti posizionati in un'orbita terrestre
media, ovvero collocata a una distanza di almeno 20.000 km dalla superficie terrestre, ma a
un'altezza non superiore a quella dell'orbita geostazionaria, che si trova a una distanza
dalla terra di circa 35.000 km ed è utilizzata dai satelliti televisivi, di comunicazioni e
Internet.
Il sistema GPS è stato originariamente progettato negli anni 60, all'apice della Guerra
fredda, per applicazioni in ambito militare e di intelligence. Nel 1983, tuttavia, quando un
aereo passeggeri coreano entrò in territorio sovietico e fu abbattuto dalle forze aree russe,
l'allora Presidente degli Stati Uniti Ronald Reagan ordinò che venisse realizzata una
versione civile del sistema GPS a disposizione di tutta la popolazione. Attualmente,
l'accesso al sistema GPS è gratuito e non prevede spese di iscrizione o di manutenzione,
ma è necessario acquistare un ricevitore GPS.
Numerosi sono gli ambiti di applicazione del sistema GPS: navigazione aeronavale, ricerca
di percorsi stradali, creazione di mappe, ricerche sui terremoti, studi climatici e persino
giochi all'aperto come il "geocaching" (la caccia al tesoro dell'era digitale).
Un ricevitore GPS standard è dotato di un'antenna sintonizzata sulle frequenze trasmesse
dai satelliti, che raccoglie il segnale e lo inoltra al processore integrato nel ricevitore, che
in questo modo è in grado di mostrare l'ora esatta e la posizione geografica.Esempi concreti di applicazione del GPS: come navigatore, sia oggetto come il tom tom o
dentro il telefono direttamente. In quest’ultimo può essere utile anche per localizzare un
cellulare perso o rubato.
Geottagging : Geotaggare indica una specifica forma di etichettatura associata a una
foto. In particolare, è l’azione attraverso la quale associamo a una fotografia il luogo dove
questa è stata scattata. Detto così, non sembra certo una novità; tutti noi quando
pubblichiamo o stampiamo una foto ci scriviamo nella didascalia il luogo dove l’abbiamo
fatta. Ma la vera innovazione sta nel fatto che questa informazione adesso può essere
salvata “dentro” il file stesso; cioè viene associata all’immagine e ne diventa una proprietà
come la data di scatto, la dimensione, il tipo di file, lo spazio colore, ecc.
Google Earth e il “buco della serratura” : Nell’Ottobre 2004 Google, compra una
piccola società, Keyhole (“buco della serratura”). Leader mondiale nel digital mapping su
Internet. Questa ha realizzato un modello 3d dell’intera superfice terrestre, continuamente
aggiornato con nuovi dati
Google prende il software di Keyhole e lo ribattezza Google Earth, ma ne cambia il
modello di business. Mentre prima si trattava di vendere a clienti professionali l’accesso in
abbonamento delle rappresentazioni cartografiche digitali, adesso Google Earth diventa
un motore di ricerca che invece do cercare siti web cerca rappresentazioni digitali di un
punto determinato della superfice terrestre.
Fotografia
Un’immagine digitale i può ottenere o realizzandola direttamente in digitale, oppure
convertendo in digitale un’immagine tradizionale, analogica, attraverso uno scanner. In
entrambe i casi l’immagine digitale può essere archiviata, modificata, inviata attraverso le
reti di telecomunicazioni, senza aver mai bisogno di stamparla e fissarla su un supporto
materiale.
Per realizzare invece un’immagine direttamente in digitale ci sono due possibilità:
disegnarla al computer con un file grafico; oppure servirsi di una macchina fotografica
digitale.
La macchina al suo interno non ha più la pellicola fotografica ma dei sensori fissi che
vengono colpiti dalla luce. Questi rilevano il colore e le caratteristiche di un minuscolo
rettangolo dell’immagine, che è loro assegnato, e lo convertono in degnale digitale. Il
risultato è il pixel. (picture element). Ogni sensore genera i dati di un pixel. L’immagine
fotografica è composta da un grandissimo numero di pixel. I pixel non li vediamo a occhio
nudo, tuttavia se ingrandiamo l’immagine si vedranno tanti rettangolini di diverso colore.
La qualità di una macchinette digitale è data dal numero di pixel che offre.Si chiama
risoluzione il numero di pixel contenuti in un pollice (PPI, pixel per inch). Più alto è il PPI,
più elevata sarà la qualità della foto.
La fotografia digitale , in quanto tale, richiederebbe , come quella tradizionale analogica,
la presenza sul luogo degli eventi del fotografo. In realtà per quanto riguarda la forografia
digitale non sempre è così. Spesso le tecniche di elaborazione e fotoritocco digitale sonocosì raffinate che permettono di creare un’immagine che assomiglia in tutto e per tutto a
una fotografia ed è indistinguibile da essa.:una mimèsi di fotografia. . Una foto digitale
può dunque essere frutto di pura invenzione o della rielaborazione di altre immagini. Si
parla in questi casi di “immagine sintetica”.
Differenze e vantaggi rispetto alla fotografia analogica:
• Si possono far tutti gli scatti che si vuole senza spendere in pellicola;
• La fotocamera è estremamente semplice ed economica;
• Non c’è lo sviluppo: si vede subito quello che abbiamo fotografato e viene
eliminato un processo lungo, costoso e dato a terzi;
• Non dovendo far sviluppare e stampare i rullini in un negozio nessuno vedrà le foto
che facciamo se non lo vogliamo;
• Ritocco e fotomontaggio ce li facciamo da soli con un bricolage al computer.
Anche la stampa è largamente accessibile al fotografo dilettante;
• La conservazione e l’invio a terzi delle foto non richiedono di stamparle. Il digitale
consente di utilizzarle per ogni altra applicazione (testi, giornali, presentazioni,
disegni ecc.);
• Con il telefonino (cameraphone) ci portiamo sempre dietro la macchina (non
perdiamo occasioni) e possiamo dissimulare lo scatto;
• Per molti usi (non tutti) la qualità delle foto amatoriali è competitiva con quelle
professionali;
• Si assottiglia la differenza tra scattare foto e girare video. Lo stesso apparecchio
permette (per video di breve durata) di scegliere se fare un video o delle foto, e di
trasformare una inquadratura video in foto.
UGC User genereted content: Molti utenti Internet amano “postare” su Internet
contenuti multimediali di loro scelta .Postare un contenuto da noi scelto denota un
maggiore protagonismo espressivo. UGC prodotti dall’utente ma non solo. Gran parte
degli USG sono incollati o rielaborati da altre fonti.
Social Network: I social network sono nuovi siti Internet in cui la partecipazione serve a:
• esprimere la propria individualità
• coltivare rapporti sociali
• generare e distribuire contenuti multimediali
Favorisce l’aggregazione in comunità e fandom, che sono anche potenti strumenti di
marketing
Fandom: comunità degli appassionati di qualcosa.Digitalizzazione del cinema:
• Minor costo e minor tempo
• Non è necessario costruire modelli (in gesso, polistirolo, silicone...)
• Tutto avviene nei laboratori (rendering farms)
• Senza spostamenti
• Senza rischi
• Non è necessario arruolare migliaia di comparse per scene di massa
• Le masse si fanno con il copia-e-incolla
• Non è necessario lavorare con animali pericolosi
Postproduzione, correzione degli errori di ripresa:
• Cavi elettrici sul pavimento
• Anacronismi nei film in costume (antenne televisive, impianti elettrici, cartelli)
• Raddrizzamento di immagini storte o ondeggianti
• Inserimento o cancellazione di oggetti
Postproduzione: montaggio digitale (editing non lineare):
Il montaggio lineare si faceva alla Moviola, oggi non più utilizzata. Si trattava di un
montaggio lineare e fisico, con pezzetti di pellicola tagliati e giuntati dal montatore, figura
chiave del montaggio in una dialettica feconda con il regista. Un film lungometraggio
montato (al netto di tutte le parti scartate) occupava più di 2 km. di pellicola.
Nel montaggio digitale, non lineare, non si taglia e non si giunta (fisicamente) nulla,
ma si utilizzano determinati programmi per il montaggio, e dunque il materiale
essendo digitale può essere modificato più e più volte senza “intaccare fisicamente”
nulla, fino alla sua versione definitiva. Il programma di montaggio è un software. Avid
Media Composer, Final Cut e Adobe Premiere sono i principali marchi del montaggio
digitale professionale.
Postproduzione: mixer audio, color correction:
Il film è composto di molteplici fonti audio che nel montaggio sono state collocate in
varie piste sonore. Ciascuna fonte audio ha tono, volume, qualità diversa e accorrearmonizzarle: è quello che si fa al mixer audio. Una fase della postproduzione
anch’essa completamente computerizzata.
Inoltre possono esserci errori nella esecuzione del suono o nella sua cattura: rumori
esterni non graditi: ad esempio il vento, colpi di tosse, sganciamenti del microfono
etc.
A tutti questi problemi si rimedia al mixer audio.
La fine della produzione della pellicola:
Fuji e Kodak sono rimasti gli ultimi produttori a livello globale di pellicola
cinematografica.
Con il 2014 la produzione è cessata definitivamente.
Distribuzione ed esercizio:
Uno dei principali problemi del film in pellicola era la sua consegna agli utenti.
La distribuzione doveva allestire un numero consistente di copie per il lancio del film. Le
copie costavano circa 3.000 euro ciascuna e, terminata la fase di lancio, erano in gran parte
inutili. Lo smaltimento delle copie inutilizzate era costoso ed ecologicamente pesante.
DCP, Digital Cinema Package:
In epoca digitale la “pizza” analogica è sostituita da un DCP di costo contenuto (meno di
100 euro), facilmente producibile e trasportabile (è grande come un hard disk e
sostanzialmente lo è).Nella gran parte dei casi la distribuzione recapita all’esercente un
DCP fisico.
Delivery virtuale :
La distribuzione cinematografica potrebbe seguire – e lo farà presto – le metodologie di
delivery proprie del video on demand.L’esercente preleva da un server dedicato la copia
che gli serve e la scarica.
La copia ha un DRM (Digital Rights Management) e l’esercente, secondo i contratti e gli
accordi di volta in volta stabiliti con la distribuzione, riceve i codici necessari alla fruizione
nei limiti e alle condizioni poste.In questo caso non c’è alcun trasferimento fisico.
Prospettiva rinascimentale:
La prospettiva è un procedimento a base geometrica e matematica per la resa in 2D della
terza dimensione, inventata a Firenze verso il 1420. La possibilità di rappresentare
efficacemente (realisticamente) la terza dimensione su una superficie piana costituisce una
grande semplificazione produttiva e una opportunità straordinaria per l’affermazione della
pittura, dell’incisione e del disegno.
La resa prospettica diventa un canone della pittura, ed è passata pari pari alla fotografia.Se noi guardiamo bene ci rendiamo conto che già in epoca pittorica, e poi nell’era della
riproduzione tecnica, ci sono molti indicatori di un superamento dei limiti della
prospettiva. La cui principale lacuna è il carattere esterno dello spettatore e la necessità
della sua collocazione frontale.
La prospettiva non è quindi una tecnica inclusiva e partecipatoria, non da all’osservatore-
spettatore l’impressione di essere parte dello show. E’ una tecnologia fredda, non
emozionale. Tutta razionale. Lontana dell’entertainment.
Barocco:
Nel barocco architettura, scultura e pittura collaborano a costituire un effetto
tridimensionale che coinvolga l’osservatore (ormai: spettatore) è lo faccia sentire interno
alla rappresentazione. Tutto l’edificio è organizzato per rendere questo effetto.
Un effetto illusionistico che è insieme inclusivo e immersivo.
Stereoscopio:
""
"
Internet of things :
In telecomunicazioni Internet delle cose (o, più propriamente, Internet degli oggetti o IoT,
acronimo dell'inglese Internet of things) è un neologismo riferito all'estensione di Internet
al mondo degli oggetti e dei luoghi concreti.
L'Internet delle cose è una possibile evoluzione dell'uso della Rete: gli oggetti (le "cose")
si rendono riconoscibili e acquisiscono intelligenza grazie al fatto di poter comunicare dati
su se stessi e accedere ad informazioni aggregate da parte di altri [7] . Le sveglie suonano
prima in caso di traffico, le scarpe da ginnastica trasmettono tempi, velocità e distanza per
gareggiare in tempo reale con persone dall'altra parte del globo, i vasetti delle medicine
avvisano i familiari se si dimentica di prendere il farmaco. Tutti gli oggetti possono
acquisire un ruolo attivo grazie al collegamento alla Rete. [8]L'obiettivo dell'internet delle cose è far sì che il mondo elettronico tracci una mappa di
quello reale, dando un'identità elettronica alle cose e ai luoghi dell'ambiente fisico. Gli
oggetti e i luoghi muniti di etichette Identificazione a radio frequenza (Rfid) o Codici QR
comunicano informazioni in rete o a dispositivi mobili come i telefoni cellulari. [9]
I campi di applicabilità sono molteplici: dalle applicazioni industriali (processi produttivi),
alla logistica e all'infomobilità, fino all'efficienza energetica, all'assistenza remota e alla
tutela ambientale
Due tipi di tecnologie:
WSN:
Con il termine Wireless Sensor Network (o WSN) si indica una determinata tipologia di
rete che, caratterizzata da una architettura distribuita, è realizzata da un insieme di
dispositivi elettronici autonomi in grado di prelevare dati dall'ambiente circostante e di
comunicare tra loro. Parametri come la pressione atmosferica, il traffico su un’autostrada,
la radioattività, il livello dell’acqua in un bacino, i passaggi degli spettatori attraverso un
varco in uno stadio sono alcuni esempi delle quantità rilevate.
RFID
La RFID (Radio Frequency Identification) è una tecnologia per il tracciamento o
l’identificazione basata su minuscolo dispositivo in genere chiamato semplicemente tag
(cartellino, etichetta)
Appartengono a questa tipologia i tag incorporati in carte di credito, banconote,
documenti, articoli merceologici, libri di una biblioteca, e che presentano molti vantaggi
rispetto ai codici a barra precedentemente e ancora largamente usati.
L’RFID, acronimo inglese di Radio-Frequency IDentification, è una tecnologia nata per
identificare automaticamente informazioni su oggetti, animali o persone attraverso
l’utilizzo di particolari etichette elettroniche, dette tag, e di appositi apparati, chiamati
reader, in grado di comunicare e aggiornare le informazioni contenute, appunto, nei tag
che sta interrogando. Per tale motivo, i reader vengono denominati anche “interrogator”,
mentre i tag “transponder”. Tutto ciò avviene mediante radiofrequenza.
I reader, pertanto, oltre a leggere le informazioni presenti dei tag RFID, sono anche in
grado ti riscriverle. In un certo senso, quindi, i dispositivi RFID possono essere equiparati a
sistemi di lettura e/o scrittura senza fili dalle molteplici applicazioni.Applicazioni RFID
I campi d’applicazione della tecnologia RFID sono numerosi. I principali, però, sono i
seguenti:
•
tracciabilità degli animali domestici e da allevamento;
•
•
•
•
•
•
•
apertura delle serrature;
per la tracciabilità dei prodotti;
tessere e documenti di identità elettronici;
carte bancarie;
titoli di viaggio elettronici;
sistemi di bigliettazione elettronica per metropolitane, treni, autobus,
in soluzioni per la mobilità (Telepass e similari) e negli interporti;
nei passaporti.
Collocato su un oggetto mobile (ma anche su un animale o una persona), il tag RFID ne
traccia gli spostamenti, ad esempio avvertendo una centrale remota quando l’oggetto
esce da un raggio di azione predeterminato. Anche la centrale remota può essere mobile:
tipicamente, uno smartphone. I dispositivi antifurto montati su questi telefoni cellulari
appartengono a questa tipologia. Un RFID può anche connettersi con una porta
disponendone la chiusura o l’apertura; ad esempio il RFID collocato sul collare di un cane
(ma anche inserito sotto la pelle) può permettere a quel cane di entrare in casa, ma non ad
altri animali.Naturalmente esistono altre applicazioni meno innocenti. Ad esempio il fornitore di un
dispositivo o di un software può essere informato, in genere – ma non necessariamente –
quando l’utente lo consente, del luogo in cui esso si trova (Location reporting) e quindi
tenere traccia non solo degli indirizzi Internet visitati, ma anche dei luoghi fisici (Location
history), utilizzando questi dati per le proprie attività ed eventualmente vendendo questi
dati ad altre applicazioni e social network che possono beneficiarne, personalizzando i
contenuti offerti in base a tali dati.
Anche RFID e WSN pongono rilevanti problemi di controllo sociale e di privacy.
ESIGENZE DI NETWORKING E DI AUTOMAZIONE
Nel loro complesso, le tecnologie digitali sopra descritte, che discendono peraltro da
tecnologie militari o spaziali, rispondono a pressanti e molteplici esigenze della società
contemporanea. Tra esse possiamo indicare, a puro titolo di esempio:
• diagnostica, medicina e assistenza a distanza di bambini, anziani, malati; aiuti e
supporti per disabili;
monitoraggio dell’ambiente, previsioni metereologiche e segnalazione precoce
degli eventi naturali (frane, slavine, movimenti tettonici), misurazione
dell’inquinamento dell’aria e dell’acqua;
• monitoraggio del pericolo di incendio (in particolare nelle foreste) e attivazione di
dispositivi antincendio;
monitoraggio di strutture e impianti anche non presenziati, controllo di apparecchi;
• controllo del traffico e della navigazione, conduzione di veicoli, loro localizzazione;
sorveglianza e allarme;
domotica (smart homes);
• monitoraggio di reti e di strutture distributive, flotte di veicoli, catene di negozi e
magazzini.
Cloud computing :
La tecnologia cloud consente di utilizzare qualsiasi tipo di documento senza aver bisogno
di chiavette Usb, hard disk e archivi digitali. Anziché sul computer, i software vengono
installati direttamente sulla rete, in una sorta di «nuvola». I dati che fino ad oggi venivano
salvati sui pc saranno decentrati su vari server: giganteschi archivi digitali a cui l’utente può
accedere grazie al browser e alle applicazioni. In concreto: invece di archiviare la nostra
musica e i nostri documenti sul computer di casa, li depositeremo su Internet e potremo
consultarli con diversi dispositivi.
Quali sono i vantaggi per gli utenti?
Rinunciando a chiavette Usb e dispositivi mobili si risparmia spazio e si viaggia leggeri. Ma
soprattutto, con la tecnologia cloud si possono creare degli archivi raggiungibili in
qualunque momento. Dalle fotografie delle vacanze ai documenti aziendali fino allecartelle sanitarie: tutto è a portata di smartphone.
Ma non è rischioso?
Il rischio più grosso è che, in caso di blackout o di incidenti ai server, i servizi siano
impossibili da raggiungere. Dunque potrebbero essere messi in pericolo i dati personali
degli utenti (dal codice della carta di credito alle coordinate bancarie) Per i sostenitori del
software libero, i problemi sarebbero invece legati alla privacy e alla censura: il potere
rischia di essere concentrato nelle mani di pochissimi grandi gruppi, in grado bloccare il
ciberspazio per le voci scomode.
Computer:
Computer è una parola di derivazione latina che sta per computare, calcolare. Per
computer, quindi, si intende un calcolatore automatico in grado di eseguire istruzioni,
fornite sotto forma di istruzioni logico-aritmetiche, e di restituire i risultati di tale
operazioni in seguito ad un processo di elaborazione.
Prima di arrivare ai computer elettronici e digitali fu necessario che il matematico inglese
Alan Turing teorizzasse il funzionamento della cosiddetta Macchina di Turing. Questo
dispositivo, prettamente ideale, processa i dati salvati su un nastro infinito secondo un
insieme definito di regole. Partendo da questo teorema, John von Neuman descrisse
l’architettura di un calcolatore, composto da un processore centrale, un’unità di memoria
su cui sono salvati sia i dati di input che i dati di output, e un bus che collega tutte le parti
tra loro. In sostanza, tutti i moderni computer sono ancora basati sulla Macchina di Turing
e sull’Architettura di von Neuman.
Mashup:
.Il termine inglese mashup (anche mash-up) è ormai comune in ambito informatico, in
particolare nello sviluppo Web, dove indica un mix di contenuto, codice o altri elementi
da fonti diverse che vengono integrati dinamicamente per creare un nuovo tipo di servizio
o applicazione. Il termine deriva da un’espressione usata in ambito musicale che indica
una tecnica che consiste nella miscela di due o più canzoni mediante l’uso di computer o
di qualsiasi altro dispositivo.
. I mashup stanno rivoluzionando lo sviluppo del web permettendo a chiunque di
combinare dati da siti come Amazon, eBay, Google, Windows Live, YouTube e Yahoo! in
modo innovativoOver the top:
L'AGCOM definisce Over-The-Top (in acronimo OTT) le imprese che forniscono, attraverso
la rete Internet, servizi, contenuti (soprattutto video) e applicazioni di tipo "rich
media" (per esempio, le pubblicità che appaiono “sopra” la pagina di un sito web mentre
lo si visita e che dopo una durata prefissata scompaiono). Esse traggono ricavo, in
prevalenza, dalla vendita di contenuti e servizi agli utenti finali (ad esempio nel caso di
Apple e del suo iTunes) o di spazi pubblicitari, come nel caso di Google e Facebook.
Broadcaster:
Il termine, mutuato dall’inglese, fa riferimento al verbo to broadcast, composto di
broad, largo, ampio, e cast, spedire, diffondere, che significa quindi letteralmente
“diffondere ampiamente” e in senso più esteso diffondere informazioni tramite il
sistema di trasmissione radiotelevisivo. Broadcaster ha quindi un doppio
significato: da un lato è l’emittente (radio o tv) che fornisce un servizio di
trasmissione, dall’altro è colui che trasmette le informazioni: il termine infatti si
utilizza anche per definire annunciatori, giornalisti e conduttori radiotelevisivi.
Crossmedialità:
Con il termine crossmedialità (o crossmedia, cross-media) ci si riferisce alla possibilità di
mettere in connessione i mezzi di comunicazione l'uno con l'altro, grazie allo sviluppo e
alla diffusione di piattaforme digitali. Un sistema che utilizza crossmedialità si definisce
"crossmediale".
Le informazioni vengono emesse, e completate, in virtù dell'interazione tra i media, per cui
assistiamo a performance comunicative nelle quali i principali mezzi di comunicazione
interagiscono fra di essi, dispiegando l'informazione nei suoi diversi formati e canali.
In questa tendenza, internet è il mezzo che meglio si adatta perché nel gioco di rinvii da
un mezzo, o un apparecchio, all'altro, spesso è coinvolto il world wide web: per esempio è
consultato in diretta nelle trasmissioni televisive; la carta stampata fornisce codici da
digitare per entrare in aree riservate dei siti web; la promozione di prodotti avviene
lanciando storie che rimpallano dall'offline all'online e viceversa, [1] la locandina o l'adesivo
su un prodotto possono avere un codice QR per permettere di leggere gli
approfondimenti via smartphone o tablet. [2] .
Per crossmedialità della comunicazione intendo la diffusione di una notizia attraverso un
vasto ventaglio di strumenti comunicativi per far sì che l’informazione raggiunga un
pubblico più ampio.
Soprattutto con l’avvento di internet, infatti, il numero degli strumenti utilizzabili per
informarsi (ed informare) ha subìto una moltiplicazione considerevole.
Un numero crescente di persone, ad esempio, non acquista giornali cartacei, ma fruisce
dei giornali on line. Sono sempre più, poi, i “netizen” (ovvero i “cittadini della rete”) chenon guardano più la televisione anche se gran parte di loro rivede le trasmissioni in diretta
streaming dal pc o in differita grazie alla TV on demand. Vi è chi invece si informa solo su
canali non “ufficiali”, come i portali e blog non giornalistici e attraverso i social network –
magari da un device mobile (smartphone o tablet) – anche perché questi strumenti
consentono ad ognuno di esprimere il proprio punto di vista interagendo con gli altri.
L’enorme flusso di informazioni che oggi arriva attraverso i vari canali ha anche cambiato le
nostre abitudini: l’approfondimento – che quando andavo a scuola io quindici – venti anni
fa era possibile solo attraverso i libri cartacei e le enciclopedie – oggi avviene grazie ai
motori di ricerca o “pretendendo” che sia la fonte stessa a venire a noi. L’indicazione dei
siti internet su una brochure, i loghi social stampigliati sul packaging degli oggetti che si
acquistano, i QR code (un “codice a barre” digitale”) che permettono di proiettare
l’interessato nell’approfondimento semplicemente attraverso l’uso dello smartphone e/o
del tablet sono esempi di integrazione crossmediale della comunicazione.
Ciò richiede due diverse abilità: conoscere gli strumenti di diffusione della comunicazione
e, al contempo, elaborare linguaggi diversi affinché, per ogni mezzo, venga usato il
linguaggio adatto al “contenitore”.
Si parla di Crossmedialità quando la comunicazione viaggia attraverso diversi media.
Un contenuto Crossmediale, prende in considerazione, vari supporti: carta stampata,
web, video, radio ecc. ottimizzando così la comunicazione. Per esempio è
crossmediale, un annuncio radiofonico che rimanda al sito internet per approfondire
l’informazione e scaricare dati.
Browser:
In informatica, il web browser è un'applicazione per la navigazione di risorse sul web. Tali
risorse (come pagine web, immagini o video) sono messe a disposizione sul World Wide
Web . Il programma implementa da un lato le funzionalità di client per il protocollo HTTP,
che regola lo scaricamento delle risorse dai server web a partire dal loro indirizzo URL;
dall'altro quelle di visualizzazione dei contenuti ipertestuali (solitamente all'interno di
documenti HTML) e di riproduzione di contenuti multimediali.
Tra i browser più utilizzati vi sono Google Chrome, Safari, Internet Explorer, Mozilla Firefox,
Opera, Microsoft Edge.
Mainframe:
Con il termine mainframe si fa riferimento ad un grande elaboratore centrale, con elevate
prestazioni in termini di capacità di calcolo o di memoria. Comunemente è usato nelle reti
come punto centrale o di smistamento, in modo da ricoprire il ruolo di server per le
migliaia di utenti che sono ad esso collegati simultaneamente.
I mainframe noti anche come server sono utilizzati nelle grandi aziende per svolgere
funzioni centralizzate, occupano un’intera stanza e sono gestiti da personale altamente
specializzato.
Supercomputer e mainframe elaborano dati immersi da migliaia di utenticontemporaneamente grazie alla costituzione di reti di computer alle quali si può
accedere secondo varie modalità.
I minicomputer sono meno potenti del mainframe e possono essere di diverse dimensioni,
spesso sono utilizzati dalle grandi aziende e dai centri di calcolo.
I mainframe e i minicomputer all’interno di un grande sistema hanno il compito di fornire i
dati richiesti dagli altri computer ed è per questo che prendono il nome d “server” che
vuol dire servire, però hanno questo nome tutti gli elaboratori che possono svolgere
questa funzione.
Digital divide:
Per Digital Divide si intende alla lettera divario, divisione digitale: esso viene inteso come
mancanza di accesso e di fruizione alle nuove tecnologie di comunicazione e informatiche.
Da qualche anno ormai si parla di questo argomento, che con il passare del tempo
riguarda aspetti sempre diversi delle nuove tecnologie e non solo: molti sono gli aspetti
anche sociali della questione.
Storicamente, i primi che parlarono di digital divide furono Al Gore e Bill Clinton, quando,
all'inizio degli anni novanta, intrapresero una politica di forte sviluppo e potenziamento
dell'infrastruttura di internet negli Stati Uniti.
Il concetto di "divario digitale" era riferito alla difficoltà di accesso a internet in
determinate zone del paese (difficoltà intesa anche sotto l'aspetto dei costi).
In quegli anni internet esplode come fenomeno di massa e diventa sempre di più un
mezzo di lavoro e di business: non essere connessi alla rete (o non avere gli strumenti
cognitivi per farlo), significa quindi essere relegati ai margini della società.
Il divario digitale [1] o digital divide è il divario esistente tra chi ha accesso effettivo alle
tecnologie dell'informazione (in particolare personal computer e internet) e chi ne è
escluso, in modo parziale o totale. I motivi di esclusione comprendono diverse variabili:
condizioni economiche, livello d'istruzione, qualità delle infrastrutture, differenze di età o
di sesso, appartenenza a diversi gruppi etnici, provenienza geografica [2] . Oltre a indicare il
divario nell'accesso reale alle tecnologie, la definizione include anche disparità
nell'acquisizione di risorse o capacità necessarie a partecipare alla società
dell'informazione. Il termine digital divide può essere utilizzato sia per riferirsi ad un
divario esistente tra diverse persone, o gruppi sociali in una stessa area, che al divario
esistente tra diverse regioni di uno stesso stato, o tra stati (o regioni del mondo) a livello
globale.
Buffer:
Memoria sulla quale vengono immagazzinati temporaneamente i dati prima di trasferirli
da un dispositivo a un altro o da un’applicazione a un’altra.
Web 1.0 – 2.0 e 3.0: l’evoluzione del web
Il Web 1.0. Era l’internet dei contenuti, i siti web erano semplici testi statici simili alle
pagine di un libro o a fogli di word. Contenevano anche immagini o video, ma lo scopo diqueste pagine era la mera consulenza, l’informazione, senza interazione fra utente e
contenuto. I siti erano formati da pagine ricche di ipertesti, pagine contenenti
collegamenti ad altre pagine, che creavano una struttura simile ad un enorme libro. Fu
proprio l’ impossibilità da parte dell’utente di interagire con i contenuti che spinse i
ricercatori a cercare un’evoluzione: rendere dinamico il web permettendo all’utente di
interagire con esso.
La trasformazione ebbe inizio con la possibilità di inserire dei commenti; in seguito con
l’ausilio di nuovi linguaggi di programmazione (php) si crearono i primi forum e i primi
blog dando vita al Web 1.5.
Ma il web non si è fermato:con lo sviluppo e l’evoluzione delle community, dei social
network, l’introduzione dei wiki (dove gli utenti possono reperire informazioni, modificarle
e aggiungerne altre, wikipedia ne è un esempio) si è spinto sempre più verso l’interattività
con l’utente dando vita a quello che attualmente è il Web 2.0 (termine coniato da Tim
O’Reilly) il web dinamico. Per la prima volta si è data grande importanza all’ usabilità e al
modo di condividere i contenuti.
Il Web 2.0 siamo noi, o almeno una parte di noi, quella che ha accettato di condividere
con altri milioni di persone (utenti) informazioni, commenti, idee attraverso post sui blog
o sui social network (Facebook, Linkedin, Twitter); video (Youtube) e fotografie (Flickr).
Il web 2.0 poggia quindi su tre pilastri: interazione, condivisione e partecipazione.
L’interazione offre a ciascun individuo la possibilità di usufruire, in tempo reale, dei
contenuti che più lo interessano e di condividerli con gli altri utenti della rete. In questo
modo la comunicazione diventa partecipativa, perché chiunque può dare il suo contributo
nella diffusione dei contenuti presenti su internet, che diventano così accessibili a tutti.
Ma il web non ha smesso di evolversi: oggi di fatto stiamo già entrando nel Web 3.0:
il web della semantica e delle cose, Web of Things.
Si parla di un unico enorme database, il WebDatabase, dove tutte le informazioni di
internet confluiranno per velocizzare ricerce e semplificare la gestione dei dati. Sarà un
web sempre più semantico perchè tutto sarà legato alle parole chiave legate ai documenti
(documenti, video, immagini, suoni) e tutte le ricerche saranno legate a queste parole. Si
parla anche di intelligenze artificiali grazie ad algoritmi sempre più sofisticati che
permetteranno un orientamento migliore in una rete sempre più affollata, che interagià
con gli utenti “quasi” fosse uno di loro. Questo potenziamento semantico unito una
macchina dotati di AI capace di interpretare come un essere umano (o quasi) una stringa:
e quindi di coglierne il significato al volo si ipotizza che i fraintendimenti di ricerca
tenderanno a scomparire.
Convergenza:
In ambito multimediale la convergenza è l'ibridazione, resa possibile dalla tecnologia
digitale, di tanti strumenti atti ad erogare informazione. Convergenza significa utilizzareuna sola interfaccia per tutti i servizi di informazione: educazione, sorveglianza,
commercio, servizi bancari, intrattenimento, ricerche, medicina, ecc.
Dagli albori della TV la ricezione dei canali sfruttava una tecnologia ANALOGICA, in cui
per via etere venivano inviati segnali e ricevuti dalle antenne tv, presenti prima nelle
abitazioni e poi sui tetti delle nostre case. La banda per la trasmissione televisiva però è
limitata (deve stare in un certo range di frequenza), per questo fino all’avvento del digitale
si era arrivati al limite massimo di canali trasmissibili. Il digitale nasce perché grazie alle
tecnologie di compressione si riescono a trasmettere a parità di banda molti più canali,
rendendo l’offerta trasmissiva più ampia.
LA TV DIGITALE NEGLI USA:
La prima TV digitale nasce negli USA nel 1994, si chiamava DIRECT TV e veniva trasmessa
via SATELLITE, rendendo necessaria una PARABOLA per la ricezione del segnale. Per
evitare che chiunque potesse vedere questi canali senza pagare semplicemente
montando una parabola i programmi digitali vengono CRIPTATI, il decoder dato in
dotazione da DIRECT TV è fornito di una SMART CARD che grazie ad una linea telefonica
decripta il segnale e permette la visione.
LA TV DIGITALE IN EUROPA E IN ITALIA:
Nel campo della tv digitale i capostipiti sono la francese CANAL PLUS e l’inglese SKY. In
Italia la TV digitale a pagamento nasce nel 1990 e si chiama TELE+, nel frattempo la STET
poco prima di diventare TELECOM, avvia un progetto di cablaggio di fibre ottiche per
tutta l’italia, associandolo ad un servizio di TV digitale a pagamento chiamato STREAM.
TELE+ ebbe poco successo dati i costi e l’offerta poco varia, STREAM a causa delle
problematiche con l’infrastruttura della fibra ottica, deve convertirsi a TV SATELLITARE.
Nel 2003 Tele+ e Stream, si unificano e vengono inglobate formando SKY ITALIA.
L’altra sponda della TV digitale è IL DIGITALE TERRESTRE, introdotto dalla legge Gasparri
del 2004. Il digitale terrestre rappresenta un’evoluzione NECESSARIA. Si tratta di
modificare l’intero sistema di trasmissione televisiva da ANALOGICO a DIGITALE. Per
questo motivo, cambiando la tecnologia di trasmissione, è stato necessario cambiare TV o
dotarsi di un DECODER esterno. Il passaggio da tv analogica a digitale è detto SWITCH
OVER, ed è stato graduale tra le regioni italiane.
Il servizio a pagamento collegato al DIGITALE TERRESTRE è MEDIASET PREMIUM,
concorrente diretto di SKY.
Il fallimento della tv interattiva: La tv digitale è stata inizialmente associata al concetto di tv
interattiva. Si pensava che un servizio in cui l’utente potesse interagire con la trasmissione
(votando o partecipando a sondaggi), in realtà il progetto è un grande flop poiché
l’avvento di internet ha di fatto reso inutile la tv interattiva.
I CONTENUTI DELLA TV DIGITALE:
La TV analogica fornisce un servizio di tipo PUSH, perché SPINGE forzatamente contenuti
verso l’utente (se su un canale c’è la partita di Champions league, in genere sugli altricanali non verranno fatte proposte di grandi show perché lo share maggiore andrà su
quell’evento)
La TV digitale fornisce un servizio di tipo PULL, poiché è lo spettatore a scegliere ciò che è
di suo gradimento nel palinsesto (anche se su un canale viene trasmessa
un’importantissima partita, su altri avremo comunque film in anteprima ecc)
Pubblicità: In oltre la TV Digitale, proprio per la sua natura di essere una TV a pagamento
ha pubblicità meno invadenti.
Notizie: La TV digitale fornisce in genere un servizio detto “all news”, ovvero un canale
tematico dedicato all’informazione disponibile h24. Il primo è stato la CNN. Con la tv
digitale satellitare SKY nasce SKY TG 24, mentre con la tv digitale terrestre nasce RAI
NEWS 24.
Sport: La TV digitale viene fortemente spinta e trova successo principalmente per la
fruizione di contenuti sportivi, grazie al digitale è possibile trasmettere in contemporanea
interi campionati. La TV ha imposto le sue esigenze ai calendari sportivi, impostando il
calcio non più come evento puramente domenicale ma facendo sì di avere sempre
qualche evento durante la settimana. In oltre la tv digitale offre ore di contenuti sportivi
dedicati a MOVIOLA, REPLAY, CALCIOMERCATO e GOSSIP.
LA TV DIGITALE VIA INTERNET:
La TV digitale oltre che alla parabola e all’antenna ha anche un atro canale di
distribuzione: INTERNET. Fastweb ha lanciato questo tipo di TV, con palinsesti
interamente trasmessi in STREAMING. I vantaggi erano notevoli poiché consentivano di
fruire di contenuti in forma ritardata e la possibilità di memorizzare e registrare
trasmissioni. Era necessaria però una connessione molto stabile e veloce con costi elevati.
Ad oggi, l’ADSL consente quasi a tutti di fruire di questi servizi. Fastweb ha chiuso il suo
progetto di TV via INTERNET e si è associata a SKY. SKY ha integrato nella sua scelta di
canali PARABOLICI, un palinsesto ON DEMAND, trasmesso via internet.
